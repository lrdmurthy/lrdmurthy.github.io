<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
	 <meta name="author" content="MURTHY L.R.D">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>MURTHY L.R.D</title>
	
	
	  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/mandelbrot_set_scaled.jpg">

    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-rbsA2VBKQhggwzxH7pPCaAqO46MgnOM80zW1RWuH61DGLwZJEdK2Kadq2F9CUG65" crossorigin="anonymous">
  </head>
  <body>
    

	<table class="table">
  <tbody>
      <div class="container text-center">
	  <div class="row justify-content-md-center">
    <div class="col col-sm-9"> 
	<tr>
      <td>
	          <p style="text-align:center">
                <name>MURTHY L R D</name>
              </p>
              <p>I am a PhD Student in I3D Lab under the supervision of <a href="https://cambum.net/PB/index.php">Dr. Pradipta Biswas</a> at Centre for Product Design and Manufacturing, <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> where I work on Human Computer Interaction, Computer Vision and Machine Learning.
              </p>
              <p>
                My Doctoral thesis is focused on developing Eye Gaze Estimation Systems using webcameras to enable gaze-controlled applications on laptops and mobile devices and Infra-Red cameras for Augmented and Virtual Reality applications.I worked briefly on Gesture Recognition, Remote manipulation of Robotic Arm and Understanding interaction in Virtual Reality. </p>

			  <p> I was fortunate enough to work as a Visiting Researcher under <a href=https://scholar.google.com/citations?hl=en&user=CO9UQcsAAAAJ&view_op=list_works&sortby=pubdate> Prof. Sriram Subramanian</a> at former Interact Lab at <a href=https://www.sussex.ac.uk> University of Sussex </a> (Currently  <a href=https://www.ucl.ac.uk/computer-science/research/research-groups/multi-sensory-devices> MSD Lab at UCL </a>). I also worked as a Research Intern in High Voltage Laboratory under <a href=https://www.unibo.it/sitoweb/andrea.cavallini/research>Prof. Andrea Cavallini</a> at University of Bologna for my Bachelor's Thesis. 

              <p> I play Badminton, I play Guitar, I read books (preferably Novels) and occasionally I write poetry in Telugu. I am Ambidextrous.</p>
              <p style="text-align:center">
                <a href="mailto:lrdmurthy@iisc.ac.in">Email</a> &nbsp/&nbsp
                <a href="data/Murthy L R D_Resume_28Jan22.pdf">CV</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp  -->
                <a href="https://scholar.google.com/citations?user=TmOU3goAAAAJ&hl=en&oi=ao">Google Scholar</a>
           <!--                     <a href="https://twitter.com/001_raghu">Twitter</a> &nbsp/&nbsp -->
           <!--     <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
			  </td>
      <td>
	   <a href="images/profile_photo_Aug29_17_Light.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_photo_Aug29_17_Light.jpg" class="hoverZoomLink"></a></td>

    </tr>
</div>    
</div>
</div>
	<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                    <li>August 2022: Our Work on <mark>Lane Detection in Unstructured Road Environments</mark> got accepted in <mark>IEEE Transactions on Artificial Intelligence.</mark> </li>
                    <li>July 2022: Our Work on Interactive Head-Up Displays using Appearance-based Gaze Tracking got accepted in <mark>AutoUI'22</mark> in Work in Progress (WiP) Track.</li>
                    <li>July 2022: Completed my <mark>PhD Colloquium</mark> on July 8th, 2022.</li>
                    <li>April 2022: Our Work on "Comparing two safe distance maintenance algorithms for a gaze controlled HRI involving users with SSMI" got accepted in <mark>Transactions on Accessible Computing</mark></li>
                    <li>Jan 2022: Two Poster Papers accepted at <mark>ACM IUI'22</mark>.</li>
                    <li>Dec 2021: One Poster Paper, accepted at <mark>ACM VRST'21</mark>, titled <a href="https://dl.acm.org/doi/abs/10.1145/3489849.3489959">Validating Social Distancing through Deep Learning and VR-Based Digital Twins</a>.</li>
                    <li>Nov 2021: Our paper, "Deep Learnng based Eye Gaze Estimation for Military Aviation" accepted for <mark>IEEE Aerospace Conference</mark>.</li>
                    <li>Aug 2021: A journal paper, "I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system" published in <mark>Journal of Eye Movement Research</mark>. check out <a href="https://bop.unibe.ch/JEMR/article/view/7667">here</a>.</li>
                    <li>Jun 2021: Our paper titled "Appearance-based Gaze Estimation using Attention and Difference Mechanism" got accepted at <a href="https://gazeworkshop.github.io/2021/">Gaze2021 workshop</a> at <mark>CVPR'21</mark></li>
                    <li>Dec 2020: Our work "Cognitive load estimation using ocular parameters in automotive" has been accepted at the <mark><a href="https://www.sciencedirect.com/science/article/pii/S2666691X20300099">Transportation Engineering</a></mark> </li>       
                    <li>Aug 2020: Our work "Webcam controlled robotic arm for persons with SSMI" appeared in <mark>Technology and Disability Journal</mark> and you can read it <a href="https://content.iospress.com/articles/technology-and-disability/tad200264">here </a></li>                  					
              </ul>
            </td>
          </tr>
		  <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
			</td>
			</tr>
<tr>
<td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/PARKS_Gaze_Sample_web_scaled.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>PARKS-Gaze - An Appearance-based Gaze Estimation Dataset in Wilder Conditions</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>,
              Abhishek Mukhopadhyay,
              Ketan Anand, Shambhavi Aggarwal, Pradipta Biswas
              <br>
        <em>ACM Intelligent User Interfaces</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 
		  
		  <tr>
		  <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/DistractionDetectionDemo_scaled.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Distraction Detection in Automotive Environment using Appearance-based Gaze Estimation</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>,
              Abhishek Mukhopadhyay, Pradipta Biswas
              <br>
        <em>ACM Intelligent User Interfaces</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This paper proposes a light-weight appearance-based gaze estimaion model which can provide real-time gaze angle predictions to determine driver visual distraction in an automotive setting.</p>
            </td>
          </tr> 
<tr>
     <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/model_architecture_ieeeaero22.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Deep Learnng based Eye Gaze Estimation for Military Aviation</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>, Pradipta Biswas
              <br>
        <em>IEEE Aerospace</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>We performed the failure mode analysis of Commercially-Off-The-Shelf IR-based wearable eye tracker and proposed an alternative deeplearning-based solution. Our solution is robust to bright external illumination conditions where the current model-based system fail.</p>
            </td>
          </tr> 
		  <tr>
		               <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/I2D-Net_Website.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>, Siddhi Brambhatt, Somnath Arjun, Pradipta Biswas
              <br>
        <em>Journal of Eye Movement Research</em>
              <br>
              <a href="https://bop.unibe.ch/JEMR/article/view/7667">VIEW</a>  
              <p></p>
              <p>We proposed an appearance-based gaze estimation system based on the feature-difference vector approach called I2D-Net. We also evaluated it for real-time gaze-controlled interaction task and found it to be useful upto for 4x4 pointing and selection task without any personalized calibration. </p>
            </td>
          </tr> 
<tr>
     <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/AGE_Net_Website.jpg' width="160">              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Appearance-based Gaze Estimation using Attention and Difference Mechanism</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>, Pradipta Biswas
              <br>
        <em>IEEE/CVF CVPR Workshops (Gaze2021), 2021</em>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.pdf">PDF</a> 
              <p></p>
              <p>I proposed two appearance-based gaze estimation models I2D-Net, based on Difference mechanism and AGE-Net based on novel weighted-features mechanism (Inspired from the famous Attention mechanism). AGE-Net achieved SOTA gaze estimaion accuracy on MPIIGaze, RT-Gene.</p>
            </td>
          </tr> 
		   <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Academic Service as Reviewer</heading>
              <ul>
                    <li>Journals: Journal of Multimedia Systems (MMSJ), Artificial Intelligence for Engineering Design, Analysis and Manufacturing (AIEDAM), Internation Journal on Social Robotics </li>
                    <li>Conferences: EuroGraphics, ACM UMAP, ACM IUI</li>
              </ul>
            </td>
          </tr>
		  
		  
		  
		  
		  
		  
		  
		  
  </tbody>
</table>
	
	
	
	
	<!--Link to the source of the website-->
                   
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://jonbarron.info/">here</a> and <a href="https://kmopuri.github.io/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
	
	<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-kenU1KFdBIe4zVF0s0G1M5b4hcpxyD9F7jL+jjXkk+Q2h455rYXK/7HAuoJl+0I4" crossorigin="anonymous"></script>
  
  
  </body>
</html>