<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>MURTHY L.R.D</title>
  
  <meta name="author" content="MURTHY L.R.D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/profile_photo_Aug29_17.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>MURTHY L R D</name>
              </p>
              <p>I am a PhD Student in I3D Lab under the supervision of <a href="https://cambum.net/PB/index.php">Dr. Pradipta Biswas</a> at Centre for Product Design and Manufacturing, <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> where I work on Human Computer Interaction, Computer Vision and Machine Learning.
              </p>
              <p>
                My Doctoral thesis is focused on developing Eye Gaze Estimation Systems using webcameras to enable gaze-controlled applications on laptops and mobile devices and Infra-Red cameras for Augmented and Virtual Reality applications.I worked briefly on Gesture Recognition, Remote manipulation of Robotic Arm and Understanding interaction in Virtual Reality. </p>

			  <p> I worked as a Visiting Researcher at former Interact Lab at <a href=https://www.sussex.ac.uk> University of Sussex </a> (Currently  <a href=https://www.ucl.ac.uk/computer-science/research/research-groups/multi-sensory-devices> MSD Lab at UCL </a>) and worked as a Research Intern in High Voltage Laboratory under <a href=https://www.unibo.it/sitoweb/andrea.cavallini/research> </a>Prof. Andrea Cavallini for my Bachelor's Thesis. 

              <p> I play Badminton, I play Guitar, I read books (preferably Novels) and occasionally I write poetry in Telugu. I am Ambidextrous.</p>
              <p style="text-align:center">
                <a href="mailto:lrdmurthy@iisc.ac.in">Email</a> &nbsp/&nbsp
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=TmOU3goAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
           <!--                     <a href="https://twitter.com/001_raghu">Twitter</a> &nbsp/&nbsp -->
           <!--     <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/JonBarron.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_photo_Aug29_17_Crop.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                    <!--  <li>Oct 2020: Our work on Early detection of stuck .</li> -->
                    <li>Jan 2022: Two Poster Papers accepted at <mark>ACM IUI'22</mark>.</li>
                    <li>Dec 2021: One Poster Paper, accepted at <mark>ACM VRST'21</mark>, titled <a href="https://dl.acm.org/doi/abs/10.1145/3489849.3489959">Validating Social Distancing through Deep Learning and VR-Based Digital Twins</a>.</li>
                    <li>Nov 2021: Our paper, "Deep Learnng based Eye Gaze Estimation for Military Aviation" accepted for <mark>IEEE Aerospace Conference</mark>.</li>
                    <li>Aug 2021: A journal paper, "I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system" published in <mark>Journal of Eye Movement Research</mark>. check out <a href="https://bop.unibe.ch/JEMR/article/view/7667">here</a>.</li>
                    <li>Jun 2021: Our paper titled "Appearance-based Gaze Estimation using Attention and Difference Mechanism" got accepted at <a href="https://gazeworkshop.github.io/2021/">Gaze2021 workshop</a> at <mark>CVPR'21</mark></li>
                    <li>Dec 2020: Our work "Cognitive load estimation using ocular parameters in automotive" has been accepted at the <mark><a href="https://www.sciencedirect.com/science/article/pii/S2666691X20300099">Transportation Engineering</a></mark> </li>       
                    <li>Dec 2020: Our work "Webcam controlled robotic arm for persons with SSMI" appeared in <mark>Technology and Disability Journal</mark> and you can read it <a href="https://content.iospress.com/articles/technology-and-disability/tad200264">here </a></li>                  
					
              </ul>
            </td>
          </tr>
        </tbody></table>


<!--    ACM IUI 2022  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='beyond-flipping.gif' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>PARKS-Gaze - An Appearance-based Gaze Estimation Dataset in Wilder Conditions</papertitle>              
              <br>
              <strong>Murthy L.R.D.*</strong>,
              Abhishek Mukhopadhyay,
              Ketan Anand, Shambhavi Aggarwal, Pradipta Biswas
              <br>
        <em>ACM Intelligent User Interfaces</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 

 </tbody></table>


 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='beyond-flipping.gif' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Distraction Detection in Automotive Environment using Appearance-based Gaze Estimation</papertitle>              
              <br>
              <strong>Murthy L.R.D.*</strong>,
              Abhishek Mukhopadhyay, Pradipta Biswas
              <br>
        <em>ACM Intelligent User Interfaces</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This paper proposes an Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 

 </tbody></table>


<!--    IEEE Aerospace 2022  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='beyond-flipping.gif' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Deep Learnng based Eye Gaze Estimation for Military Aviation</papertitle>              
              <br>
              <strong>Murthy L.R.D.*</strong>, Pradipta Biswas
              <br>
        <em>IEEE Aerospace</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 

 </tbody></table>

<!--    JEMR-->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='beyond-flipping.gif' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system</papertitle>              
              <br>
              <strong>Murthy L.R.D.*</strong>, Siddhi Brambhatt, Somnath Arjun, Pradipta Biswas
              <br>
        <em>Journal of Eye Movement Research</em>
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 

 </tbody></table>


<!--    CVPR 21-->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='beyond-flipping.gif' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Appearance-based Gaze Estimation using Attention and Difference Mechanism</papertitle>              
              <br>
              <strong>Murthy L.R.D.*</strong>, Pradipta Biswas
              <br>
        <em>IEEE/CVF CVPR Workshops (Gaze2021), 2021</em>
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 

 </tbody></table>







<!--Link to the source of the website-->
                   
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://jonbarron.info/">here and <a href="https://kmopuri.github.io/"here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>






