<!DOCTYPE html>
<html lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <title>MURTHY L.R.D</title>
  <meta name="author" content="MURTHY L.R.D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/mandelbrot_set_scaled.jpg">
</head>
<body>
  <h1 class="h1_3sle9d">MURTHY L R D</h1>
  <table class="table_qpc5t2">
    <tbody>
      <tr class="tr_3l2kez">
        <td class="td_kkzeyk">
          <table class="table_jgzeuk">
            <tbody>
              <tr class="tr_925gm4">
                <td class="td_5p8yzu">
                  <p>I am a PhD Student in I3D Lab under the supervision of <a href="https://cambum.net/PB/index.php">Dr. Pradipta Biswas</a> at Centre for Product Design and Manufacturing, <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> where I work on Human Computer Interaction, Computer Vision and Machine Learning.</p>
                  <p>My Doctoral thesis is focused on developing Eye Gaze Estimation Systems using webcameras to enable gaze-controlled applications on laptops and mobile devices and Infra-Red cameras for Augmented and Virtual Reality applications.I worked briefly on Gesture Recognition, Remote manipulation of Robotic Arm and Understanding interaction in Virtual Reality.</p>
                  <p>I was fortunate enough to work as a Visiting Researcher under <a href="https://scholar.google.com/citations?hl=en&amp;user=CO9UQcsAAAAJ&amp;view_op=list_works&amp;sortby=pubdate">Prof. Sriram Subramanian</a> at former Interact Lab at <a href="https://www.sussex.ac.uk">University of Sussex</a> (Currently <a href="https://www.ucl.ac.uk/computer-science/research/research-groups/multi-sensory-devices">MSD Lab at UCL</a> ). I also worked as a Research Intern in High Voltage Laboratory under <a href="https://www.unibo.it/sitoweb/andrea.cavallini/research">Prof. Andrea Cavallini</a> at University of Bologna for my Bachelor&apos;s Thesis.</p>
                  <p>I play Badminton, I play Guitar, I read books (preferably Novels) and occasionally I write poetry in Telugu. I am Ambidextrous.</p>
                  <p class="p_hoon9y"><a href="mailto:lrdmurthy@iisc.ac.in">Email</a> &#xA0;/&#xA0; <a href="data/Murthy%20L%20R%20D_Resume.pdf">CV</a> &#xA0;/&#xA0; <!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp  -->
                   <a href="https://scholar.google.com/citations?user=TmOU3goAAAAJ&amp;hl=en&amp;oi=ao">Google Scholar</a> <!--                     <a href="https://twitter.com/001_raghu">Twitter</a> &nbsp/&nbsp -->
                   <!--     <a href="https://github.com/jonbarron/">Github</a> --></p>
                </td>
                <td class="td_7o6akj">
                  <a href="images/profile_photo_Aug29_17_Light.jpg"><img alt="profile photo" src="images/profile_photo_Aug29_17_Light.jpg" class="hoverZoomLink img_32285l"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table_k0is55">
            <tbody>
              <tr>
                <td class="td_1n87os">
                  <h3>News</h3>
                  <ul>
                    <li>October 2022: <mark>Submitted my Thesis. Actively looking for postdocs at the interscetion of HCI and AI including topics like Computer Vision for HCI, Human-AI Interaction, Developing novel and accessible interfaces etc., </mark> </li>
					<li>August 2022: Our Work on <mark>Lane Detection in Unstructured Road Environments</mark> got accepted in <mark>IEEE Transactions on Artificial Intelligence.</mark></li>
                    <li>July 2022: Our Work on Interactive Head-Up Displays using Appearance-based Gaze Tracking got accepted in <mark>AutoUI&apos;22</mark> in Work in Progress (WiP) Track.</li>
                    <li>July 2022: Completed my <mark>PhD Colloquium</mark> on July 8th, 2022.</li>
                    <li>April 2022: Our Work on &quot;Comparing two safe distance maintenance algorithms for a gaze controlled HRI involving users with SSMI&quot; got accepted in <mark>Transactions on Accessible Computing</mark></li>
                    <li>Jan 2022: Two Poster Papers accepted at <mark>ACM IUI&apos;22</mark>.</li>
                    <li>Dec 2021: One Poster Paper, accepted at <mark>ACM VRST&apos;21</mark>, titled <a href="https://dl.acm.org/doi/abs/10.1145/3489849.3489959">Validating Social Distancing through Deep Learning and VR-Based Digital Twins</a>.
                    </li>
                    <li>Nov 2021: Our paper, &quot;Deep Learnng based Eye Gaze Estimation for Military Aviation&quot; accepted for <mark>IEEE Aerospace Conference</mark>.</li>
                    <li>Aug 2021: A journal paper, &quot;I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system&quot; published in <mark>Journal of Eye Movement Research</mark>. check out <a href="https://bop.unibe.ch/JEMR/article/view/7667">here</a>.
                    </li>
                    <li>Jun 2021: Our paper titled &quot;Appearance-based Gaze Estimation using Attention and Difference Mechanism&quot; got accepted at <a href="https://gazeworkshop.github.io/2021/">Gaze2021 workshop</a> at <mark>CVPR&apos;21</mark>
                    </li>
                    <li>Dec 2020: Our work &quot;Cognitive load estimation using ocular parameters in automotive&quot; has been accepted at the <mark><a href="https://www.sciencedirect.com/science/article/pii/S2666691X20300099">Transportation Engineering</a></mark></li>
                    <li>Aug 2020: Our work &quot;Webcam controlled robotic arm for persons with SSMI&quot; appeared in <mark>Technology and Disability Journal</mark> and you can read it <a href="https://content.iospress.com/articles/technology-and-disability/tad200264">here</a>
                    </li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table_plhzf8">
            <tbody>
              <tr>
                <td class="td_bmefkq">
                  <h3>Selected Publications</h3>
                </td>
              </tr>
            </tbody>
          </table><!--    Arxiv'22  -->

          <table class="table_po2z80">
            <tbody>
              <tr>
                <td class="td_8j8ann"><img src="images/parksgaze_precision.gif" width="160" alt="precisionwithparksgaze"></td>
                <td class="td_lx7b7q">	
			Towards Precision in Appearance-based Gaze Estimation in the Wild<br>
                  <strong>Murthy L.R.D.</strong>, Abhishek Mukhopadhyay, Ketan Anand, Shambhavi Aggarwal, Pradipta Biswas<br>
                 <!--  <em>IEEE Transactions on Artificial Intelligence</em><br> -->
                  <a href="https://arxiv.org/abs/2302.02353">VIEW</a>
                  <!--              <a href="https://link.springer.com/chapter/10.1007/978-3-031-24670-8_40">PDF</a>  -->
                  <p>This work extends the earlier proposed PARKS-Gaze dataset with larger nuber of samples and participants. It investigates the precision, along with accuracy of existing gaze estimation models. Takeaway result from the paper is training dataset influences the precision of gaze estimation model and the proposed dataset reduces precision error, even under cross-dataset evaluation settings.</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table class="table_po2z80">
            <tbody>
              <tr>
                <td class="td_8j8ann"><img src="images/AutoUI_Img.JPG" width="160" alt="AutoUI_InteractiveHUD_eyetracking"></td>
                <td class="td_lx7b7q">
			Efficient Interaction with Automotive Heads-Up Displays using Appearance-based Gaze Tracking<br>
                  <strong>Murthy L.R.D.</strong>, Gyanig Kumar, Modiksha Madan, Sachin Deshmukh, Pradipta Biswas<br>
                  <em>ACM Automotive User Interfaces (AutoUI)</em>, 2022<br>
                  <a href="https://dl.acm.org/doi/abs/10.1145/3544999.3554818">VIEW</a>
                  <!--              <a href="https://link.springer.com/chapter/10.1007/978-3-031-24670-8_40">PDF</a>  -->
                  <p>This paper proposes an interactive Heads-Up Display for automotive using webcam-based eye tracking system. We compared the proposed approach with gesture-based interaction and presented both quantitative and qualitative results.</p>
                </td>
              </tr>
            </tbody>
          </table>

          <table class="table_po2z80">
            <tbody>
              <tr>
                <td class="td_8j8ann"><img src="images/lane_detection_whole.gif" width="160" alt="lanedetection"></td>
                <td class="td_lx7b7q">	
			A Hybrid Lane Detection Model for Wild Road Conditions<br>
                  Abhishek Mukhopadhyay, <strong>Murthy L.R.D.</strong>, Imon Mukherjee, Pradipta Biswas<br>
                  <em>IEEE Transactions on Artificial Intelligence</em><br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/9913325">VIEW</a>
                  <!--              <a href="https://link.springer.com/chapter/10.1007/978-3-031-24670-8_40">PDF</a>  -->
                  <p>This work presents an Indian Lane Dataset (ILD), augmenting existing Indian Driving Dataset (IDD) with lane marking labels and proposes a novel hybrid CNN architecture with a new loss function for accurate lane detection results, even in unstructured road scenarios.</p>
                </td>
              </tr>
            </tbody>
          </table>



          <table class="table_po2z80">
            <tbody>
              <tr>
                <td class="td_8j8ann"><img src="images/JoystickCar_Using_EyeTracking_Scaled.jpg" width="160" alt="JoystickCarEyeTracking_scaled"></td>
                <td class="td_lx7b7q">
                  Enabling Learning Through Play: Inclusive Gaze-Controlled Human-Robot Interface for Joystick-Based Toys<br>
                  Vinay Krishna Sharma, <strong>Murthy L.R.D.</strong>, Pradipta Biswas<br>
                  <em>International Conference on Social Robotics</em>, 2022<br>
                  <a href="https://link.springer.com/chapter/10.1007/978-3-031-24670-8_40">VIEW</a>
                  <p>This paper investigates the utility of a gamified approach to introduce gaze tracking technology to people with sever speech and motor impairment. Results indicate that exposure to a fun activity like controlling a joystick-based control using eye tracking increases the user engagement time for learning activities with eye tracking. This work was done using earlier proposed webcam-based gaze estimation models and the PARKS-Gaze dataset</p>
                </td>
              </tr>
            </tbody>
          </table>


          <table class="table_po2z80">
            <tbody>
              <tr>
                <td class="td_8j8ann"><img src="images/PARKS_Gaze_Sample_web_scaled.png" width="160" alt="PARKS_Gaze_Sample_web_scaled"></td>
                <td class="td_lx7b7q">
                  PARKS-Gaze - An Appearance-based Gaze Estimation Dataset in Wilder Conditions<br>
                  <strong>Murthy L.R.D.</strong>, Abhishek Mukhopadhyay, Ketan Anand, Shambhavi Aggarwal, Pradipta Biswas<br>
                  <em>ACM Intelligent User Interfaces</em>, 2022<br>
				  <a href="https://dl.acm.org/doi/abs/10.1145/3490100.3516467">VIEW</a>

                  <!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
                  <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table_fugdf3">
            <tbody>
              <tr>
                <td class="td_n32egf"><img src="images/DistractionDetectionDemo_scaled.png" width="160" alt="DistractionDetectionDemo_scaled"></td>
                <td class="td_9jq4r3">
                  Distraction Detection in Automotive Environment using Appearance-based Gaze Estimation<br>
                  <strong>Murthy L.R.D.</strong>, Abhishek Mukhopadhyay, Pradipta Biswas<br>
                  <em>ACM Intelligent User Interfaces</em>, 2022<br>
				  <a href="https://dl.acm.org/doi/abs/10.1145/3490100.3516463">VIEW</a>

                  <!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
                  <p>This paper proposes a light-weight appearance-based gaze estimaion model which can provide real-time gaze angle predictions to determine driver visual distraction in an automotive setting.</p>
                </td>
              </tr>
            </tbody>
          </table><!--    IEEE Aerospace 2022  -->
          <table class="table_ic0onc">
            <tbody>
              <tr>
                <td class="td_d9cqvy"><img src="images/model_architecture_ieeeaero22.png" width="160" alt="model_architecture_ieeeaero22"></td>
                <td class="td_y43is5">
                  Deep Learnng based Eye Gaze Estimation for Military Aviation<br>
                  <strong>Murthy L.R.D.</strong>, Pradipta Biswas<br>
                  <em>IEEE Aerospace</em>, 2022<br>
				  <a href="https://ieeexplore.ieee.org/abstract/document/9843506">VIEW</a>
                  <!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
                  <p>We performed the failure mode analysis of Commercially-Off-The-Shelf IR-based wearable eye tracker and proposed an alternative deeplearning-based solution. Our solution is robust to bright external illumination conditions where the current model-based system fail.</p>
                </td>
              </tr>
            </tbody>
          </table><!--    JEMR-->
          <table class="table_qi5yy0">
            <tbody>
              <tr>
                <td class="td_iisbme"><img src="images/I2D-Net_Website.jpg" width="160" alt="I2D-Net_Website"></td>
                <td class="td_fizjjv">
                  I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system<br>
                  <strong>Murthy L.R.D.</strong>, Siddhi Brambhatt, Somnath Arjun, Pradipta Biswas<br>
                  <em>Journal of Eye Movement Research</em><br>
                  <a href="https://bop.unibe.ch/JEMR/article/view/7667">VIEW</a>
                  <p>We proposed an appearance-based gaze estimation system based on the feature-difference vector approach called I2D-Net. We also evaluated it for real-time gaze-controlled interaction task and found it to be useful upto for 4x4 pointing and selection task without any personalized calibration.</p>
                </td>
              </tr>
            </tbody>
          </table><!--    CVPR 21-->
          <table class="table_prfhit">
            <tbody>
              <tr>
                <td class="td_jaqidn"><img src="images/AGE_Net_Website.jpg" width="160" alt="AGE_Net_Website"></td>
                <td class="td_psq0uj">
                  Appearance-based Gaze Estimation using Attention and Difference Mechanism<br>
                  <strong>Murthy L.R.D.</strong>, Pradipta Biswas<br>
                  <em>IEEE/CVF CVPR Workshops (Gaze2021), 2021</em><br>
                  <a href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.pdf">PDF</a>
                  <p>I proposed two appearance-based gaze estimation models I2D-Net, based on Difference mechanism and AGE-Net based on novel weighted-features mechanism (Inspired from the famous Attention mechanism). AGE-Net achieved SOTA gaze estimaion accuracy on MPIIGaze, RT-Gene.</p>
                </td>
              </tr>
            </tbody>
          </table>
          <table class="table_1aruew">
            <tbody>
              <tr>
                <td class="td_1jphz0">
                 <h3> Academic Service as Reviewer </h3>
                  <ul>
                    <li>Journals: IEEE Sensors, Journal of Multimedia Systems (MMSJ), Artificial Intelligence for Engineering Design, Analysis and Manufacturing (AIEDAM), Internation Journal on Social Robotics</li>
                    <li>Conferences: ACM CHI’23, ACM IUI’23, EuroGraphics, ACM UMAP, ACM IUI</li>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table><!--Link to the source of the website-->
          <table class="table_8qksh2">
            <tbody>
              <tr>
                <td class="td_owhrj7">
                  <br>
                  <p class="p_sqbq5m">Source taken from <a href="https://jonbarron.info/">here</a> and <a href="https://kmopuri.github.io/">here</a>.</p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
    </tbody>
  </table>
</body>
</html>
