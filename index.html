<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>MURTHY L.R.D</title>
  
  <meta name="author" content="MURTHY L.R.D">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/mandelbrot_set_scaled.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>MURTHY L R D</name>
              </p>
              <p>I am a PhD Student in I3D Lab under the supervision of <a href="https://cambum.net/PB/index.php">Dr. Pradipta Biswas</a> at Centre for Product Design and Manufacturing, <a href="https://iisc.ac.in/">Indian Institute of Science, Bangalore</a> where I work on Human Computer Interaction, Computer Vision and Machine Learning.
              </p>
              <p>
                My Doctoral thesis is focused on developing Eye Gaze Estimation Systems using webcameras to enable gaze-controlled applications on laptops and mobile devices and Infra-Red cameras for Augmented and Virtual Reality applications.I worked briefly on Gesture Recognition, Remote manipulation of Robotic Arm and Understanding interaction in Virtual Reality. </p>

			  <p> I was fortunate enough to work as a Visiting Researcher under <a href=https://scholar.google.com/citations?hl=en&user=CO9UQcsAAAAJ&view_op=list_works&sortby=pubdate> Prof. Sriram Subramanian</a> at former Interact Lab at <a href=https://www.sussex.ac.uk> University of Sussex </a> (Currently  <a href=https://www.ucl.ac.uk/computer-science/research/research-groups/multi-sensory-devices> MSD Lab at UCL </a>). I also worked as a Research Intern in High Voltage Laboratory under <a href=https://www.unibo.it/sitoweb/andrea.cavallini/research>Prof. Andrea Cavallini</a> at University of Bologna for my Bachelor's Thesis. 

              <p> I play Badminton, I play Guitar, I read books (preferably Novels) and occasionally I write poetry in Telugu. I am Ambidextrous.</p>
              <p style="text-align:center">
                <a href="mailto:lrdmurthy@iisc.ac.in">Email</a> &nbsp/&nbsp
                <a href="data/Murthy L R D_Resume_28Jan22.pdf">CV</a> &nbsp/&nbsp
<!--                <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp  -->
                <a href="https://scholar.google.com/citations?user=TmOU3goAAAAJ&hl=en&oi=ao">Google Scholar</a>
           <!--                     <a href="https://twitter.com/001_raghu">Twitter</a> &nbsp/&nbsp -->
           <!--     <a href="https://github.com/jonbarron/">Github</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/profile_photo_Aug29_17_Light.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile_photo_Aug29_17_Light.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
              <ul>
                    <li>August 2022: Our Work on <mark>Lane Detection in Unstructured Road Environments</mark> got accepted in <mark>IEEE Transactions on Artificial Intelligence.</mark> </li>
                    <li>July 2022: Our Work on Interactive Head-Up Displays using Appearance-based Gaze Tracking got accepted in AutoUI'22 in Work in Progress (WiP) Track.</li>
                    <li>July 2022: Completed my <mark>PhD Colloquium</mark> on July 8th, 2022.</li>
                    <li>Jan 2022: Two Poster Papers accepted at <mark>ACM IUI'22</mark>.</li>
                    <li>Dec 2021: One Poster Paper, accepted at <mark>ACM VRST'21</mark>, titled <a href="https://dl.acm.org/doi/abs/10.1145/3489849.3489959">Validating Social Distancing through Deep Learning and VR-Based Digital Twins</a>.</li>
                    <li>Nov 2021: Our paper, "Deep Learnng based Eye Gaze Estimation for Military Aviation" accepted for <mark>IEEE Aerospace Conference</mark>.</li>
                    <li>Aug 2021: A journal paper, "I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system" published in <mark>Journal of Eye Movement Research</mark>. check out <a href="https://bop.unibe.ch/JEMR/article/view/7667">here</a>.</li>
                    <li>Jun 2021: Our paper titled "Appearance-based Gaze Estimation using Attention and Difference Mechanism" got accepted at <a href="https://gazeworkshop.github.io/2021/">Gaze2021 workshop</a> at <mark>CVPR'21</mark></li>
                    <li>Dec 2020: Our work "Cognitive load estimation using ocular parameters in automotive" has been accepted at the <mark><a href="https://www.sciencedirect.com/science/article/pii/S2666691X20300099">Transportation Engineering</a></mark> </li>       
                    <li>Aug 2020: Our work "Webcam controlled robotic arm for persons with SSMI" appeared in <mark>Technology and Disability Journal</mark> and you can read it <a href="https://content.iospress.com/articles/technology-and-disability/tad200264">here </a></li>                  					
              </ul>
            </td>
          </tr>
        </tbody></table>



        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Selected Publications</heading>
			</td>
			</tr>
			</tbody>
			</table>

<!--    ACM IUI 2022  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/PARKS_Gaze_Sample_web_scaled.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>PARKS-Gaze - An Appearance-based Gaze Estimation Dataset in Wilder Conditions</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>,
              Abhishek Mukhopadhyay,
              Ketan Anand, Shambhavi Aggarwal, Pradipta Biswas
              <br>
        <em>ACM Intelligent User Interfaces</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This Appearance-based Gaze Estimaion Dataset focuses on two things which earlier datasets did not address, Precision and Wide Head Pose Angles. This is the largest in-the-wild appearance-based gaze estimation dataset in terms of total number of frames collected.</p>
            </td>
          </tr> 

 </tbody></table>


 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/DistractionDetectionDemo_scaled.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Distraction Detection in Automotive Environment using Appearance-based Gaze Estimation</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>,
              Abhishek Mukhopadhyay, Pradipta Biswas
              <br>
        <em>ACM Intelligent User Interfaces</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>This paper proposes a light-weight appearance-based gaze estimaion model which can provide real-time gaze angle predictions to determine driver visual distraction in an automotive setting.</p>
            </td>
          </tr> 

 </tbody></table>


<!--    IEEE Aerospace 2022  -->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/model_architecture_ieeeaero22.png' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Deep Learnng based Eye Gaze Estimation for Military Aviation</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>, Pradipta Biswas
              <br>
        <em>IEEE Aerospace</em>, 2022  
              <br>
<!--              <a href="https://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Mopuri_Adversarial_Fooling_Beyond_Flipping_the_Label_CVPRW_2020_paper.pdf">PDF</a>  -->
              <p></p>
              <p>We performed the failure mode analysis of Commercially-Off-The-Shelf IR-based wearable eye tracker and proposed an alternative deeplearning-based solution. Our solution is robust to bright external illumination conditions where the current model-based system fail.</p>
            </td>
          </tr> 

 </tbody></table>

<!--    JEMR-->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
         
             <td style="padding:20px;width:25%;vertical-align:middle">
              
                  <img src='images/I2D-Net_Website.jpg' width="160">
              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>I2DNet - Design and real-time evaluation of an appearance-based gaze estimation system</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>, Siddhi Brambhatt, Somnath Arjun, Pradipta Biswas
              <br>
        <em>Journal of Eye Movement Research</em>
              <br>
              <a href="https://bop.unibe.ch/JEMR/article/view/7667">VIEW</a>  
              <p></p>
              <p>We proposed an appearance-based gaze estimation system based on the feature-difference vector approach called I2D-Net. We also evaluated it for real-time gaze-controlled interaction task and found it to be useful upto for 4x4 pointing and selection task without any personalized calibration. </p>
            </td>
          </tr> 

 </tbody></table>


<!--    CVPR 21-->
 <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <td style="padding:20px;width:25%;vertical-align:middle">
                  <img src='images/AGE_Net_Website.jpg' width="160">              
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">              
                <papertitle>Appearance-based Gaze Estimation using Attention and Difference Mechanism</papertitle>              
              <br>
              <strong>Murthy L.R.D.</strong>, Pradipta Biswas
              <br>
        <em>IEEE/CVF CVPR Workshops (Gaze2021), 2021</em>
              <br>
              <a href="https://openaccess.thecvf.com/content/CVPR2021W/GAZE/papers/D_Appearance-Based_Gaze_Estimation_Using_Attention_and_Difference_Mechanism_CVPRW_2021_paper.pdf">PDF</a> 
              <p></p>
              <p>I proposed two appearance-based gaze estimation models I2D-Net, based on Difference mechanism and AGE-Net based on novel weighted-features mechanism (Inspired from the famous Attention mechanism). AGE-Net achieved SOTA gaze estimaion accuracy on MPIIGaze, RT-Gene.</p>
            </td>
          </tr> 

 </tbody></table>



<!--Link to the source of the website-->
                   
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Source taken from <a href="https://jonbarron.info/">here</a> and <a href="https://kmopuri.github.io/">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>






